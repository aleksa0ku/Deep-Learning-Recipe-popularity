{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3db4604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data saved to community_recipes.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Function to scrape a single recipe\n",
    "def scrape_recipe(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extracting the required information\n",
    "    recipe_title = soup.find('h1').text.strip()\n",
    "\n",
    "    # Extracting recipe rating\n",
    "    rating_elements = soup.find_all('li', class_='rc-rate-star')\n",
    "    recipe_rating = sum(1 for star in rating_elements if 'rc-rate-star-full' in star['class'])\n",
    "\n",
    "    # Extracting global likes\n",
    "    global_likes_button = soup.find('button', {'data-test': 'global-likes'})\n",
    "    global_likes = global_likes_button.find('span').text.strip() if global_likes_button and global_likes_button.find('span') else 'No likes'\n",
    "\n",
    "    # Extracting difficulty\n",
    "    difficulty = soup.find('span', {'data-test': 'recipe-difficulty-level'}).text.strip() if soup.find('span', {'data-test': 'recipe-difficulty-level'}) else 'No difficulty'\n",
    "\n",
    "    # Extracting preparation, baking, and resting times\n",
    "    time_elements = soup.find_all('div', {'class': 'text-base font-semibold sm:text-lg'})\n",
    "    \n",
    "    preparation = time_elements[0].text.strip() if len(time_elements) > 0 else 'No preparation time'\n",
    "    baking = time_elements[1].text.strip() if len(time_elements) > 1 else 'No baking time'\n",
    "    resting = time_elements[2].text.strip() if len(time_elements) > 2 else 'No resting time'\n",
    "\n",
    "    # Extracting ingredients\n",
    "    ingredients_elements = soup.find_all('div', {'data-test': 'recipe-ingredients-item'})\n",
    "    ingredients = []\n",
    "    for element in ingredients_elements:\n",
    "        amount_element = element.find('div', {'data-test': 'recipe-ingredients-item-amount'})\n",
    "        name_element = element.find('div', {'class': 'flex-1'})\n",
    "        link_element = element.find('a', {'class': 'global-link'})\n",
    "        \n",
    "        if amount_element and (name_element or link_element):\n",
    "            amount = amount_element.text.strip()\n",
    "            name = name_element.text.strip() if name_element else link_element.text.strip()\n",
    "            ingredients.append(f\"{amount} {name}\")\n",
    "\n",
    "    # Extracting steps\n",
    "    steps_elements = soup.find_all('p', class_='text-pretty')\n",
    "    steps = [step.text.strip() for step in steps_elements]\n",
    "\n",
    "    # Extracting tags\n",
    "    tags_elements = soup.find_all('li', {'data-test': 'recipe-tags-item'})\n",
    "    tags = [tag.find('a').text.strip() for tag in tags_elements]\n",
    "\n",
    "    # Extracting nutrition information\n",
    "    nutrition = {\n",
    "        'Cal': 'N/A',\n",
    "        'Fat': 'N/A',\n",
    "        'Protein': 'N/A',\n",
    "        'Carb': 'N/A'\n",
    "    }\n",
    "    nutrition_labels = soup.find_all('span', class_='font-semibold')\n",
    "    for element in nutrition_labels:\n",
    "        parent = element.find_parent('div')\n",
    "        label = parent.find('span').text.strip()\n",
    "        value = element.text.strip()\n",
    "        if label == 'Cal':\n",
    "            nutrition['Cal'] = value\n",
    "        elif label == 'Fat':\n",
    "            nutrition['Fat'] = value\n",
    "        elif label == 'Protein':\n",
    "            nutrition['Protein'] = value\n",
    "        elif label == 'Carb':\n",
    "            nutrition['Carb'] = value\n",
    "\n",
    "    recipe_data = {\n",
    "        'recipe_title': recipe_title,\n",
    "        'recipe_rating': recipe_rating,\n",
    "        'global_likes': global_likes,\n",
    "        'difficulty': difficulty,\n",
    "        'preparation': preparation,\n",
    "        'baking': baking,\n",
    "        'resting': resting,\n",
    "        'ingredients': ingredients,\n",
    "        'steps': steps,\n",
    "        'tags': tags,\n",
    "        'Cal': nutrition['Cal'],\n",
    "        'Fat': nutrition['Fat'],\n",
    "        'Protein': nutrition['Protein'],\n",
    "        'Carb': nutrition['Carb']\n",
    "    }\n",
    "    \n",
    "    return recipe_data\n",
    "\n",
    "# Function to scroll and load all recipes\n",
    "def load_all_recipes(driver):\n",
    "    len_of_page = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight); return document.body.scrollHeight;\")\n",
    "    match = False\n",
    "    while not match:\n",
    "        last_count = len_of_page\n",
    "        time.sleep(2)\n",
    "        len_of_page = driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight); return document.body.scrollHeight;\")\n",
    "        match = last_count == len_of_page\n",
    "\n",
    "# Function to scrape all recipes from the community page\n",
    "def scrape_community_recipes(url):\n",
    "    driver = webdriver.Chrome()  # Make sure chromedriver is in your PATH\n",
    "    driver.get(url)\n",
    "    load_all_recipes(driver)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    driver.quit()\n",
    "\n",
    "    # Find all the recipe links on the community page\n",
    "    recipe_links = [a['href'] for a in soup.find_all('a', {'data-test': 'global-card-title'})]\n",
    "\n",
    "    # Base URL to append relative links\n",
    "    base_url = \"https://www.kitchenstories.com\"\n",
    "\n",
    "    recipes = []\n",
    "    for link in recipe_links:\n",
    "        recipe_url = f\"{base_url}{link}\"\n",
    "        recipe_data = scrape_recipe(recipe_url)\n",
    "        recipes.append(recipe_data)\n",
    "    \n",
    "    return recipes\n",
    "\n",
    "# URL of the community recipes page\n",
    "community_url = \"https://www.kitchenstories.com/en/recipes/community\"\n",
    "\n",
    "# Scrape the community recipes\n",
    "community_recipes = scrape_community_recipes(community_url)\n",
    "\n",
    "# Save the recipes to a CSV file\n",
    "csv_file = \"community_recipes.csv\"\n",
    "csv_columns = ['recipe_title', 'recipe_rating', 'global_likes', 'difficulty', 'preparation', 'baking', 'resting', 'ingredients', 'steps', 'tags', 'Cal', 'Fat', 'Protein', 'Carb']\n",
    "\n",
    "with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "    writer.writeheader()\n",
    "    for recipe in community_recipes:\n",
    "        # Flatten the lists into strings for CSV\n",
    "        recipe['ingredients'] = ', '.join(recipe['ingredients'])\n",
    "        recipe['steps'] = ' | '.join(recipe['steps'])\n",
    "        recipe['tags'] = ', '.join(recipe['tags'])\n",
    "        writer.writerow(recipe)\n",
    "\n",
    "print(f\"Scraped data saved to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6585e6fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
